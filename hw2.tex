\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{fancyvrb}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{mathrsfs}
\usepackage{tzplot}
\usepackage{tikz}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{titlesec}
\usepackage{lmodern}
\usepackage{etoolbox}
\usepackage{csquotes}
\usepackage{xcolor}
\usepackage[shortlabels]{enumitem}
%\usepackage{lipsum}
\usepackage{accents}
\usepackage{minted}

%stat macro
\newcommand{\ut}[1]{\underaccent{\tilde}{#1}}
\renewcommand{\vec}[1]{\ut{#1}}
\newcommand{\ind}{\stackrel{\text{ind}}{\sim}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\cas}{\stackrel{\text{a.s}}{\rightarrow}}
\newcommand{\cp}{\stackrel{p}{\rightarrow}}
\newcommand{\cd}{\stackrel{d}{\rightarrow}}
\newcommand{\nx}{x_1,\dots,x_n}
\newcommand{\ny}{y_1,\dots,y_n}
\newcommand{\Done}[2]{\text{#1}\left({#2}\right)}
\newcommand{\Dtwo}[3]{\text{#1}\left({#2},{#3}\right)}
\newcommand{\cn}{\mathcal{N}}

\def\tmp#1 #2\relax{#1}
\setbox0=\hbox{$\xdef\intfont{%
    \expandafter\tmp\fontname\textfont3\expandafter\space\space\relax}$}
\font\tmp=\intfont\space at10pt\relax
\setbox0=\hbox{$\textfont3=\tmp \displaystyle \int$}
\dimen0=\ht0 \advance\dimen0 by\dp0 \divide\dimen0 by10 
\xdef\intsize{\the\dimen0}

\def\dividedimen (#1/#2){\expandafter\ignorept\the
   \dimexpr\numexpr\number\dimexpr#1\relax
   *65536/\number\dimexpr#2\relax\relax sp\relax
}
{\lccode`\?=`\p \lccode`\!=`\t  \lowercase{\gdef\ignorept#1?!{#1}}}

\def\flexibleint{\def\fxintL{}\def\fxintU{}\futurelet\next\fxintA}
\def\fxintA{\ifx\next_\expandafter\fxintB\else\expandafter\fxintC\fi}
\def\fxintB_#1{\def\fxintL{#1}\fxintC}
\def\fxintC{\futurelet\next\fxintD}
\def\fxintD{\ifx\next^\expandafter\fxintE\else\expandafter\fxintF\fi}
\def\fxintE^#1{\def\fxintU{#1}\fxintF}
\def\fxintF#1{\begingroup
   \setbox0=\hbox{$\displaystyle{#1}$}%
   \dimen0=\ht0 \advance\dimen0 by\dp0
   \setbox1=\hbox{$\vcenter{\copy0}$}%
   \font\tmp=\intfont\space at\dividedimen(\dimen0/\intsize)pt
   \lower\dimexpr\dp0-\dp1\hbox{%
      $\textfont3=\tmp \displaystyle\int_{\fxintL}^{\fxintU}$}
   \box0
   \endgroup
}


\makeatletter
\patchcmd{\section}{-3.5ex \@plus -1ex \@minus -.2ex}{-3.5ex \@plus -1ex \@minus -.2ex\setlength{\leftskip}{0cm}}{}{}
\patchcmd{\subsection}{-3.25ex\@plus -1ex \@minus -.2ex}{3.25ex\@plus -1ex \@minus -.2ex\setlength{\leftskip}{0cm}}{}{}
\patchcmd{\subsection}{1.5ex \@plus .2ex}{1.5ex \@plus .2ex\setlength{\leftskip}{2cm}}{}{}
\makeatother
\titleformat{\section}
  {\normalfont\fontsize{12}{15}\bfseries}{\thesection}{1em}{}
\pagestyle{fancy}
\fancyhf{}
\rhead{Aukkawut Ammartayakun}
\lhead{MA 556 Applied Bayesian Statistics: Homework 2}
\cfoot{\thepage}
\title{Homework 2}
\author{Aukkawut Ammartayakun\\MA 556 Applied Bayesian Statistics}
\date{Spring 2024}
\newcommand{\vtt}[1]{%
  \text{\normalfont\ttfamily\detokenize{#1}}%
}
\begin{document}

\maketitle
\noindent
\Large{\textbf{Problem 1}}\normalsize
\\


It is believed that $35\%$ of Americans belong to the democratic party. In an opinion poll it was found that $20\%$ of those belonging to the democratic party voted yes and $60\%$ of those belonging to other parties voted yes. Suppose someone selected at random voted yes, can you tell whether she/he is a democratic?

\vspace{\baselineskip}
\noindent
\textbf{Solution}

Let $D$ be an event that a random person is democratic, and $Y = 1$ means that person vote yes for the poll and $Y = 0$ means otherwise. Then,
\[P(D) = 0.35, \quad P(Y = 1|D) = 0.20, \quad P(Y = 1|D^{'}) = 0.60 \]
Then,
\begin{align*}
    P(D|Y=1) &= \frac{P(Y=1|D)P(D)}{P(Y=1|D)P(D) + P(Y=1|D^{'}) P(D^{'})}\\
    &= \frac{(0.20)(0.35)}{(0.20)(0.35) + (0.60)(0.65)}\\
    &= 0.1522
\end{align*}
Thus, we can say that with the probability of 0.1522, that person is democratic.
\vspace{\baselineskip}

\noindent
\Large{\textbf{Problem 2}}\normalsize
\\


I tossed a coin 25 times independently and I got 25 heads. What does this
information tell you about the next toss?  [Hint: You can assume a priori that the probability of heads is a uniform random variable.]


\vspace{\baselineskip}
\noindent
\textbf{Solution}

Let $H$ be our prior and assume that $H\sim \text{Unif}(0,1)$. Then, let $E$ be our evidence (number of heads. in this case, 25 heads). This essentially follows the model
\begin{align*}
    E|H,p &\sim \text{Binom}(n,p)\\
    p &\sim \text{Unif}(0,1)
\end{align*}
Here, this is Binomial-Beta conjugate (as $\text{Unif}(0,1)$ is equivalent to $\text{Beta}(1,1)$), and the result should be Beta distribution. To show that, assume that $p\sim \text{Beta}(\alpha,\beta)$, then
\begin{align*}
    H|E = x &\propto {n\choose x} p^{x}(1-p)^{n-x} \frac{p^{\alpha-1}(1-p)^{\beta - 1}}{B(\alpha,\beta)}\\
    &\propto p^{x+\alpha -1} (1-p)^{n-x+\beta - 1}\\
    &\sim \text{Beta}(x+\alpha, n-x+\beta)
\end{align*}
In this case, $x = 25$, $n = 25$, $\alpha = \beta = 1$. Then, $H|E = 25 \sim \text{Beta}(26,1)$. That means, the probability that this will be head given the evidence is $\mathbb{E}[H|E] = \frac{26}{27} = 0.963$.

\newpage
\noindent
\Large{\textbf{Problem 3}}\normalsize
\\

Explain the importance of the likelihood principle. Why can Bayesians and
non-Bayesians disagree about the likelihood principle? What should your stand be? Is the mode (point of maximum) of a likelihood function a random variable?

\vspace{\baselineskip}
\noindent
\textbf{Solution}

The likelihood principle states that, once the data have been observed, all the information about the parameters is contained in the likelihood function. In the Bayesian sense, you need to get the data to be able to use this principle, while non-Bayesian can assume hypothetical data (might/might not occur) to infer the parameters, which are considered incoherent in Bayesian's view. I would stick with the Bayesian one. And if we have the data, mode of a likelihood would be constant but only that we have the data.

\vspace{\baselineskip}
\noindent
\Large{\textbf{Problem 4}}\normalsize
\\

The joint density of $x_1,\dots x_n$ is
\[f(\vec{x}) = \frac{\beta^{\alpha}\Gamma(n\bar{x} + \alpha)}{\{\prod_{i=1}^n x!\}\Gamma(\alpha)(\beta + n)^{n\bar{x} + \alpha}}, \; x = 0,1,\dots\]

where $\alpha$ and $\beta$ are fixed known numbers.
\begin{enumerate}
    \item Are $\nx$ independent? Are $\nx$ identical? Are $\nx$ exchangeable?
    \item Find a random variable $y$ such that given $y$, $\nx$ are independent and identically distributed.
\end{enumerate}

[Hint: Consider Poisson and Gamma distributions.]

\vspace{\baselineskip}
\noindent
\textbf{Solution}

\begin{enumerate}
    \item $\nx$ are exchangeable because, in this density, each $x_i$ operates under the operation with permutation-invariant (product for the denominator and addition in the mean). Thus, it is exchangeable and identical. However, it does not say anything about independent.
    \item Consider the case of the Poisson-Gamma mixture
    \begin{align*}
    \nx | \lambda &\iid \Done{Poisson}{\lambda}\\
    \lambda &\sim \Dtwo{Gamma}{\alpha}{\beta}
    \end{align*}
    Then, joint density is
    \begin{align*}
        f(\vec{x}|\lambda) &= \frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-\beta \lambda} \prod_{i=1}^n \frac{\lambda^{x_i}e^{-\lambda}}{x_i!}\\
        f(\vec{x}|\lambda) &= \frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{(1-\beta)\lambda}\frac{\lambda^{n\bar{x}}}{\{\prod_{i=1}^n x_i!\}}\\
        f(\vec{x}) &= \int_{0}^{\infty} \frac{\beta^{\alpha}}{\Gamma(\alpha)}\lambda^{\alpha -1}e^{-(n+\beta)\lambda}\frac{\lambda^{n\bar{x}}}{\{\prod_{i=1}^n x_i!\}} \; d\lambda\\
        f(\vec{x}) &=  \frac{\beta^{\alpha}}{\Gamma(\alpha){\{\prod_{i=1}^n x_i!\}}} \int_{0}^{\infty}\lambda^{n\bar{x} + \alpha -1}e^{-(n+\beta)\lambda} \; d\lambda\\
        f(\vec{x}) &= \frac{\beta^{\alpha}}{\Gamma(\alpha){\{\prod_{i=1}^n x_i!\}}}\frac{\Gamma(n\bar{x} + \alpha)}{(\beta+n)^{n\bar{x}+\alpha}}
    \end{align*}
    Thus, $y\sim\Dtwo{Gamma}{\alpha}{\beta}$ makes this distribution iid.
\end{enumerate}

\vspace{\baselineskip}
\noindent
\Large{\textbf{Problem 5}}\normalsize
\\

Let 
\begin{align*}
   \ny, y_{n+1}, \dots y_N | \mu & \ind\mathcal{N}(\mu,\sigma^2)\\
    \mu &\sim \mathcal{N}(0,\delta^2)
\end{align*}
Find the posterior density of $\bar{Y} = \frac{1}{N}\sum_{i=1}^N y_i$ if $\ny$ are observed (data).


\vspace{\baselineskip}
\noindent
\textbf{Solution}

We already know from the lecture that the posterior distribution $\mu|\vec{y} \sim \mathcal{N}\left(\frac{
\delta^2}{\delta^2 + \frac{\sigma^{2}}{n}} \bar{y}, \left(1-\frac{
\delta^2}{\delta^2 + \frac{\sigma^{2}}{n}}\right)\delta^2\right)$
and we also know that $y_{n+1}, \dots y_N | \vec{y}$ is also normal with same mean and variance of $\sigma^2 +\left(1-\frac{
\delta^2}{\delta^2 + \frac{\sigma^{2}}{n}}\right)\delta^2$. Thus, the posterior density of $\bar{Y}$ is a mean transformed of that said distribution which will result in
$\bar{Y}|\vec{y} \sim \mathcal{N}\left(\frac{N}{N}\frac{\delta^2}{\delta^2 + \frac{\sigma^{2}}{n}} \bar{y}, \frac{N}{N^2}\left(\sigma^2 +\left(1-\frac{\delta^2}{\delta^2 +\frac{\sigma^{2}}{n}}\right)\delta^2\right)\right)$

\vspace{\baselineskip}
\noindent
\Large{\textbf{Problem 6}}\normalsize
\\

Let $\ny | \theta \ind \Done{Bernoulli}{\theta}, \theta \sim \Dtwo{Unif}{0}{1}$. Let $\phi = \ln\left(\frac{\theta}{1-\theta}\right)$. Find a
simulation consistent estimator of $\mathbb{E}[\phi|\vec{y}]$. Suppose $n = 100$ and $\sum_{i=1}^n y_i =25$, find the estimate and give its standard error.


\vspace{\baselineskip}
\noindent
\textbf{Solution}

The posterior distribution $\theta | \vec{y}$ can be viewed similarly to problem 2 which is essentially 
\[\theta | \vec{y} \sim \Dtwo{Beta}{1+\sum_{i=1}^n y_i}{1+n-\sum_{i=1}^ny_i}\]
So, if we sample $\theta_1,\dots \theta_m$ from this distribution (or more specifically, $\Dtwo{Beta}{26}{76}$), we can construct the simulation consistent estimator
\[\frac{1}{m}\sum_{j=1}^m \phi(\theta_i) \cas \mathbb{E}[\phi|\vec{y}]\]
This logit transformed distribution is then be
\begin{align*}
    \phi(\theta|\alpha,\beta) &= f(\phi^{-1}(\theta)|\alpha,\beta)\left|\frac{d\phi^{-1}(\theta)}{d\theta}\right|\\
    &=\frac{\left(\frac{e^\theta}{1 + e^\theta}\right)^{\alpha - 1}\left(1 - \frac{e^\theta}{1 + e^\theta}\right)^{\beta - 1}}{B(\alpha, \beta)} \cdot \frac{e^\theta}{(1 + e^\theta)^2}
\end{align*}
Then, the analytical variance would be 
\[Var(\phi(\theta)) = \flexibleint_{-\infty}^{\infty}{\theta^2 \frac{\left(\frac{e^\theta}{1 + e^\theta}\right)^{\alpha - 1}\left(1 - \frac{e^\theta}{1 + e^\theta}\right)^{\beta - 1}}{B(\alpha, \beta)} \cdot \frac{e^\theta}{(1 + e^\theta)^2} d\theta} - \left(\flexibleint_{-\infty}^{\infty}{\theta\frac{\left(\frac{e^\theta}{1 + e^\theta}\right)^{\alpha - 1}\left(1 - \frac{e^\theta}{1 + e^\theta}\right)^{\beta - 1}}{B(\alpha, \beta)} \cdot \frac{e^\theta}{(1 + e^\theta)^2} d\theta}\right)^2\]
and the standard error of this estimator then can be approximated with $SE\left(\frac{1}{m}\sum_{j=1}^m \phi(\theta_i)\right) = \sqrt{\frac{Var(\phi(\theta))}{m}}$. For the finite sample ($m=1000$), the approximation of variance with sample variance and the standard error will be $\frac{0.223}{\sqrt{1000}} = 0.0071$. 
\newpage
\section*{Appendix}
\noindent
\textbf{Code for Problem 6}
\begin{minted}{python}
import numpy as np
from scipy.stats import beta
#define parameters
alpha = 26
beta_param = 76
num_samples = 1000
#drawing samples
theta_samples = beta.rvs(alpha, beta_param, size=num_samples)
#logit transform
phi_samples = np.log(theta_samples / (1 - theta_samples))
#SD and SE
standard_deviation = np.std(phi_samples, ddof=1)
standard_error = standard_deviation / np.sqrt(num_samples)
#Print it out
print(f"Standard Deviation: {standard_deviation}")
print(f"Standard Error: {standard_error}")
\end{minted}
\end{document} 
